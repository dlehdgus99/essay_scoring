{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8310ae9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T03:17:28.348263Z",
     "iopub.status.busy": "2024-06-26T03:17:28.347588Z",
     "iopub.status.idle": "2024-06-26T03:18:06.387331Z",
     "shell.execute_reply": "2024-06-26T03:18:06.386241Z"
    },
    "papermill": {
     "duration": 38.050773,
     "end_time": "2024-06-26T03:18:06.389847",
     "exception": false,
     "start_time": "2024-06-26T03:17:28.339074",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textstat\r\n",
      "  Downloading textstat-0.7.3-py3-none-any.whl.metadata (14 kB)\r\n",
      "Collecting pyphen (from textstat)\r\n",
      "  Downloading pyphen-0.15.0-py3-none-any.whl.metadata (3.3 kB)\r\n",
      "Downloading textstat-0.7.3-py3-none-any.whl (105 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pyphen-0.15.0-py3-none-any.whl (2.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: pyphen, textstat\r\n",
      "Successfully installed pyphen-0.15.0 textstat-0.7.3\r\n",
      "Collecting pyspellchecker\r\n",
      "  Downloading pyspellchecker-0.8.1-py3-none-any.whl.metadata (9.4 kB)\r\n",
      "Downloading pyspellchecker-0.8.1-py3-none-any.whl (6.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: pyspellchecker\r\n",
      "Successfully installed pyspellchecker-0.8.1\r\n",
      "Collecting vaderSentiment\r\n",
      "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from vaderSentiment) (2.31.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->vaderSentiment) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->vaderSentiment) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->vaderSentiment) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->vaderSentiment) (2024.2.2)\r\n",
      "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: vaderSentiment\r\n",
      "Successfully installed vaderSentiment-3.3.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install textstat\n",
    "!pip install pyspellchecker\n",
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba858479",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-06-26T03:18:06.408758Z",
     "iopub.status.busy": "2024-06-26T03:18:06.408396Z",
     "iopub.status.idle": "2024-06-26T03:18:16.550770Z",
     "shell.execute_reply": "2024-06-26T03:18:16.549753Z"
    },
    "papermill": {
     "duration": 10.154235,
     "end_time": "2024-06-26T03:18:16.553100",
     "exception": false,
     "start_time": "2024-06-26T03:18:06.398865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /usr/share/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from textstat import textstat\n",
    "import spacy\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from spellchecker import SpellChecker\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c028405",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T03:18:16.571388Z",
     "iopub.status.busy": "2024-06-26T03:18:16.570923Z",
     "iopub.status.idle": "2024-06-26T03:18:16.575894Z",
     "shell.execute_reply": "2024-06-26T03:18:16.574958Z"
    },
    "papermill": {
     "duration": 0.016334,
     "end_time": "2024-06-26T03:18:16.577986",
     "exception": false,
     "start_time": "2024-06-26T03:18:16.561652",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f94a2f51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T03:18:16.595393Z",
     "iopub.status.busy": "2024-06-26T03:18:16.595121Z",
     "iopub.status.idle": "2024-06-26T03:18:20.341019Z",
     "shell.execute_reply": "2024-06-26T03:18:20.340169Z"
    },
    "papermill": {
     "duration": 3.75708,
     "end_time": "2024-06-26T03:18:20.343279",
     "exception": false,
     "start_time": "2024-06-26T03:18:16.586199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed data exists\n"
     ]
    }
   ],
   "source": [
    "train_data = None\n",
    "train_dir = \"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv\"\n",
    "processed_data_dir = '/kaggle/input/aes-data-more-feature/aes_train_data_processed'\n",
    "\n",
    "if os.path.exists(processed_data_dir):\n",
    "    print('processed data exists')\n",
    "    train_data = pickle.load(open(processed_data_dir, 'rb'))\n",
    "else:\n",
    "    train_data = pd.read_csv(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d5cc3cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T03:18:20.362015Z",
     "iopub.status.busy": "2024-06-26T03:18:20.361437Z",
     "iopub.status.idle": "2024-06-26T03:18:20.561000Z",
     "shell.execute_reply": "2024-06-26T03:18:20.560067Z"
    },
    "papermill": {
     "duration": 0.210982,
     "end_time": "2024-06-26T03:18:20.563159",
     "exception": false,
     "start_time": "2024-06-26T03:18:20.352177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>score</th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <th>flesch_kincaid_grade</th>\n",
       "      <th>smog_index</th>\n",
       "      <th>coleman_liau_index</th>\n",
       "      <th>automated_readability_index</th>\n",
       "      <th>dale_chall_readability_score</th>\n",
       "      <th>difficult_words</th>\n",
       "      <th>...</th>\n",
       "      <th>std_neutral</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>readability</th>\n",
       "      <th>num_clauses</th>\n",
       "      <th>avg_clause_length</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>pos_counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000d118</td>\n",
       "      <td>Many people have car where they live. The thin...</td>\n",
       "      <td>3</td>\n",
       "      <td>57.98</td>\n",
       "      <td>14.7</td>\n",
       "      <td>11.7</td>\n",
       "      <td>8.19</td>\n",
       "      <td>18.3</td>\n",
       "      <td>8.74</td>\n",
       "      <td>60</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090188</td>\n",
       "      <td>545</td>\n",
       "      <td>13</td>\n",
       "      <td>41.923077</td>\n",
       "      <td>57.98</td>\n",
       "      <td>13</td>\n",
       "      <td>41.923077</td>\n",
       "      <td>248</td>\n",
       "      <td>[(Many, JJ), (people, NNS), (have, VBP), (car,...</td>\n",
       "      <td>{'JJ': 33, 'NNS': 36, 'VBP': 28, 'NN': 74, 'WR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fe60</td>\n",
       "      <td>I am a scientist at NASA that is discussing th...</td>\n",
       "      <td>3</td>\n",
       "      <td>87.55</td>\n",
       "      <td>5.4</td>\n",
       "      <td>6.8</td>\n",
       "      <td>4.99</td>\n",
       "      <td>6.2</td>\n",
       "      <td>6.31</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074660</td>\n",
       "      <td>371</td>\n",
       "      <td>19</td>\n",
       "      <td>19.526316</td>\n",
       "      <td>87.55</td>\n",
       "      <td>20</td>\n",
       "      <td>18.550000</td>\n",
       "      <td>168</td>\n",
       "      <td>[(I, PRP), (am, VBP), (a, DT), (scientist, NN)...</td>\n",
       "      <td>{'PRP': 24, 'VBP': 16, 'DT': 40, 'NN': 43, 'IN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001ab80</td>\n",
       "      <td>People always wish they had the same technolog...</td>\n",
       "      <td>4</td>\n",
       "      <td>65.15</td>\n",
       "      <td>9.9</td>\n",
       "      <td>11.5</td>\n",
       "      <td>8.94</td>\n",
       "      <td>11.6</td>\n",
       "      <td>7.24</td>\n",
       "      <td>67</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100791</td>\n",
       "      <td>605</td>\n",
       "      <td>24</td>\n",
       "      <td>25.208333</td>\n",
       "      <td>65.15</td>\n",
       "      <td>25</td>\n",
       "      <td>24.200000</td>\n",
       "      <td>243</td>\n",
       "      <td>[(People, NNS), (always, RB), (wish, VBP), (th...</td>\n",
       "      <td>{'NNS': 39, 'RB': 51, 'VBP': 23, 'PRP': 20, 'V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001bdc0</td>\n",
       "      <td>We all heard about Venus, the planet without a...</td>\n",
       "      <td>4</td>\n",
       "      <td>58.32</td>\n",
       "      <td>10.4</td>\n",
       "      <td>13.2</td>\n",
       "      <td>10.97</td>\n",
       "      <td>12.9</td>\n",
       "      <td>8.50</td>\n",
       "      <td>78</td>\n",
       "      <td>...</td>\n",
       "      <td>0.126816</td>\n",
       "      <td>511</td>\n",
       "      <td>21</td>\n",
       "      <td>24.333333</td>\n",
       "      <td>58.32</td>\n",
       "      <td>21</td>\n",
       "      <td>24.333333</td>\n",
       "      <td>241</td>\n",
       "      <td>[(We, PRP), (all, DT), (heard, VBP), (about, I...</td>\n",
       "      <td>{'PRP': 19, 'DT': 49, 'VBP': 10, 'IN': 63, 'NN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>002ba53</td>\n",
       "      <td>Dear, State Senator\\n\\nThis is a letter to arg...</td>\n",
       "      <td>3</td>\n",
       "      <td>54.66</td>\n",
       "      <td>11.8</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.57</td>\n",
       "      <td>13.9</td>\n",
       "      <td>7.79</td>\n",
       "      <td>55</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150539</td>\n",
       "      <td>418</td>\n",
       "      <td>15</td>\n",
       "      <td>27.866667</td>\n",
       "      <td>54.66</td>\n",
       "      <td>16</td>\n",
       "      <td>26.125000</td>\n",
       "      <td>156</td>\n",
       "      <td>[(Dear, NNP), (,, ,), (State, NNP), (Senator, ...</td>\n",
       "      <td>{'NNP': 34, ',': 13, 'DT': 67, 'VBZ': 24, 'NN'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  essay_id                                          full_text  score  \\\n",
       "0  000d118  Many people have car where they live. The thin...      3   \n",
       "1  000fe60  I am a scientist at NASA that is discussing th...      3   \n",
       "2  001ab80  People always wish they had the same technolog...      4   \n",
       "3  001bdc0  We all heard about Venus, the planet without a...      4   \n",
       "4  002ba53  Dear, State Senator\\n\\nThis is a letter to arg...      3   \n",
       "\n",
       "   flesch_reading_ease  flesch_kincaid_grade  smog_index  coleman_liau_index  \\\n",
       "0                57.98                  14.7        11.7                8.19   \n",
       "1                87.55                   5.4         6.8                4.99   \n",
       "2                65.15                   9.9        11.5                8.94   \n",
       "3                58.32                  10.4        13.2               10.97   \n",
       "4                54.66                  11.8        13.0               10.57   \n",
       "\n",
       "   automated_readability_index  dale_chall_readability_score  difficult_words  \\\n",
       "0                         18.3                          8.74               60   \n",
       "1                          6.2                          6.31               24   \n",
       "2                         11.6                          7.24               67   \n",
       "3                         12.9                          8.50               78   \n",
       "4                         13.9                          7.79               55   \n",
       "\n",
       "   ...  std_neutral  num_words  num_sentences  avg_sentence_length  \\\n",
       "0  ...     0.090188        545             13            41.923077   \n",
       "1  ...     0.074660        371             19            19.526316   \n",
       "2  ...     0.100791        605             24            25.208333   \n",
       "3  ...     0.126816        511             21            24.333333   \n",
       "4  ...     0.150539        418             15            27.866667   \n",
       "\n",
       "   readability  num_clauses  avg_clause_length  unique_words  \\\n",
       "0        57.98           13          41.923077           248   \n",
       "1        87.55           20          18.550000           168   \n",
       "2        65.15           25          24.200000           243   \n",
       "3        58.32           21          24.333333           241   \n",
       "4        54.66           16          26.125000           156   \n",
       "\n",
       "                                            pos_tags  \\\n",
       "0  [(Many, JJ), (people, NNS), (have, VBP), (car,...   \n",
       "1  [(I, PRP), (am, VBP), (a, DT), (scientist, NN)...   \n",
       "2  [(People, NNS), (always, RB), (wish, VBP), (th...   \n",
       "3  [(We, PRP), (all, DT), (heard, VBP), (about, I...   \n",
       "4  [(Dear, NNP), (,, ,), (State, NNP), (Senator, ...   \n",
       "\n",
       "                                          pos_counts  \n",
       "0  {'JJ': 33, 'NNS': 36, 'VBP': 28, 'NN': 74, 'WR...  \n",
       "1  {'PRP': 24, 'VBP': 16, 'DT': 40, 'NN': 43, 'IN...  \n",
       "2  {'NNS': 39, 'RB': 51, 'VBP': 23, 'PRP': 20, 'V...  \n",
       "3  {'PRP': 19, 'DT': 49, 'VBP': 10, 'IN': 63, 'NN...  \n",
       "4  {'NNP': 34, ',': 13, 'DT': 67, 'VBZ': 24, 'NN'...  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f81dd99a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T03:18:20.584770Z",
     "iopub.status.busy": "2024-06-26T03:18:20.584046Z",
     "iopub.status.idle": "2024-06-26T03:18:20.591937Z",
     "shell.execute_reply": "2024-06-26T03:18:20.591090Z"
    },
    "papermill": {
     "duration": 0.020314,
     "end_time": "2024-06-26T03:18:20.593761",
     "exception": false,
     "start_time": "2024-06-26T03:18:20.573447",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def textstat_features(text):\n",
    "    features = {}\n",
    "    features['flesch_reading_ease'] = textstat.flesch_reading_ease(text)\n",
    "    features['flesch_kincaid_grade'] = textstat.flesch_kincaid_grade(text)\n",
    "    features['smog_index'] = textstat.smog_index(text)\n",
    "    features['coleman_liau_index'] = textstat.coleman_liau_index(text)\n",
    "    features['automated_readability_index'] = textstat.automated_readability_index(\n",
    "        text)\n",
    "    features['dale_chall_readability_score'] = textstat.dale_chall_readability_score(\n",
    "        text)\n",
    "    features['difficult_words'] = textstat.difficult_words(text)\n",
    "    features['linsear_write_formula'] = textstat.linsear_write_formula(text)\n",
    "    features['gunning_fog'] = textstat.gunning_fog(text)\n",
    "    features['text_standard'] = textstat.text_standard(text, float_output=True)\n",
    "    features['spache_readability'] = textstat.spache_readability(text)\n",
    "    features['mcalpine_eflaw'] = textstat.mcalpine_eflaw(text)\n",
    "    features['reading_time'] = textstat.reading_time(text)\n",
    "    features['syllable_count'] = textstat.syllable_count(text)\n",
    "    features['lexicon_count'] = textstat.lexicon_count(text)\n",
    "    features['monosyllabcount'] = textstat.monosyllabcount(text)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7675b6a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T03:18:20.612573Z",
     "iopub.status.busy": "2024-06-26T03:18:20.612306Z",
     "iopub.status.idle": "2024-06-26T03:18:20.617374Z",
     "shell.execute_reply": "2024-06-26T03:18:20.616590Z"
    },
    "papermill": {
     "duration": 0.016725,
     "end_time": "2024-06-26T03:18:20.619247",
     "exception": false,
     "start_time": "2024-06-26T03:18:20.602522",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def spelling_features(text):\n",
    "    spell = SpellChecker()\n",
    "\n",
    "    features = {}\n",
    "\n",
    "    words = nltk.word_tokenize(text)\n",
    "    misspelled = spell.unknown(words)\n",
    "\n",
    "    misspelled_count = len(misspelled)\n",
    "    misspelled_ratio = misspelled_count / len(words)\n",
    "\n",
    "    features['misspelled_count'] = misspelled_count\n",
    "    features['misspelled_count'] = misspelled_ratio\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7487c2ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T03:18:20.638851Z",
     "iopub.status.busy": "2024-06-26T03:18:20.638595Z",
     "iopub.status.idle": "2024-06-26T03:18:20.651859Z",
     "shell.execute_reply": "2024-06-26T03:18:20.651026Z"
    },
    "papermill": {
     "duration": 0.024888,
     "end_time": "2024-06-26T03:18:20.653715",
     "exception": false,
     "start_time": "2024-06-26T03:18:20.628827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_linguistic_features(text):\n",
    "\n",
    "    doc = nlp(text)\n",
    "    features = {}\n",
    "\n",
    "    # tense features\n",
    "    tenses = [i.morph.get(\"Tense\") for i in doc]\n",
    "    tenses = [i[0] for i in tenses if i]\n",
    "    tense_counts = Counter(tenses)\n",
    "    features['past_tense_ratio'] = tense_counts.get(\n",
    "        \"Past\", 0) / (tense_counts.get(\"Pres\", 0) + tense_counts.get(\"Past\", 0) + 1e-5)\n",
    "    features['present_tense_ratio'] = tense_counts.get(\n",
    "        \"Pres\", 0) / (tense_counts.get(\"Pres\", 0) + tense_counts.get(\"Past\", 0) + 1e-5)\n",
    "\n",
    "    # len features\n",
    "\n",
    "    features['word_count'] = len(doc)\n",
    "    features['sentence_count'] = len([sentence for sentence in doc.sents])\n",
    "    features['words_per_sentence'] = features['word_count'] / \\\n",
    "        features['sentence_count']\n",
    "    features['std_words_per_sentence'] = np.std(\n",
    "        [len(sentence) for sentence in doc.sents])\n",
    "\n",
    "    features['unique_words'] = len(set([token.text for token in doc]))\n",
    "    features['lexical_diversity'] = features['unique_words'] / \\\n",
    "        features['word_count']\n",
    "\n",
    "    paragraph = text.split('\\n\\n')\n",
    "\n",
    "    features['paragraph_count'] = len(paragraph)\n",
    "\n",
    "    features['avg_chars_by_paragraph'] = np.mean(\n",
    "        [len(paragraph) for paragraph in paragraph])\n",
    "    features['avg_words_by_paragraph'] = np.mean(\n",
    "        [len(nltk.word_tokenize(paragraph)) for paragraph in paragraph])\n",
    "    features['avg_sentences_by_paragraph'] = np.mean(\n",
    "        [len(nltk.sent_tokenize(paragraph)) for paragraph in paragraph])\n",
    "\n",
    "    # sentiment features\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "    compound_scores, negative_scores, positive_scores, neutral_scores = [], [], [], []\n",
    "    for sentence in sentences:\n",
    "        scores = analyzer.polarity_scores(sentence)\n",
    "        compound_scores.append(scores['compound'])\n",
    "        negative_scores.append(scores['neg'])\n",
    "        positive_scores.append(scores['pos'])\n",
    "        neutral_scores.append(scores['neu'])\n",
    "\n",
    "    features[\"mean_compound\"] = np.mean(compound_scores)\n",
    "    features[\"mean_negative\"] = np.mean(negative_scores)\n",
    "    features[\"mean_positive\"] = np.mean(positive_scores)\n",
    "    features[\"mean_neutral\"] = np.mean(neutral_scores)\n",
    "\n",
    "    features[\"std_compound\"] = np.std(compound_scores)\n",
    "    features[\"std_negative\"] = np.std(negative_scores)\n",
    "    features[\"std_positive\"] = np.std(positive_scores)\n",
    "    features[\"std_neutral\"] = np.std(neutral_scores)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2e147d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T03:18:20.672199Z",
     "iopub.status.busy": "2024-06-26T03:18:20.671962Z",
     "iopub.status.idle": "2024-06-26T03:18:20.679154Z",
     "shell.execute_reply": "2024-06-26T03:18:20.678341Z"
    },
    "papermill": {
     "duration": 0.018493,
     "end_time": "2024-06-26T03:18:20.680989",
     "exception": false,
     "start_time": "2024-06-26T03:18:20.662496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_features(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "\n",
    "    # Length-based features\n",
    "    num_words = len(tokens)\n",
    "    num_sentences = textstat.sentence_count(text)\n",
    "    avg_sentence_length = num_words / num_sentences if num_sentences != 0 else 0\n",
    "\n",
    "    # Readability features\n",
    "    readability = textstat.flesch_reading_ease(text)\n",
    "\n",
    "    # Text complexity\n",
    "    doc = nlp(text)\n",
    "    num_clauses = sum([1 for token in doc if token.dep_ == 'ROOT'])\n",
    "    avg_clause_length = num_words / num_clauses if num_clauses != 0 else 0\n",
    "\n",
    "    # Text variation\n",
    "    unique_words = len(set(tokens))\n",
    "    pos_counts = nltk.FreqDist(tag for (word, tag) in pos_tags)\n",
    "\n",
    "\n",
    "    features = {\n",
    "        'num_words': num_words,\n",
    "        'num_sentences': num_sentences,\n",
    "        'avg_sentence_length': avg_sentence_length,\n",
    "        'readability': readability,\n",
    "        'num_clauses': num_clauses,\n",
    "        'avg_clause_length': avg_clause_length,\n",
    "        'unique_words': unique_words,\n",
    "        'pos_tags': pos_tags,\n",
    "        'pos_counts': dict(pos_counts),\n",
    "    }\n",
    "\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c95d9e09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T03:18:20.699571Z",
     "iopub.status.busy": "2024-06-26T03:18:20.699104Z",
     "iopub.status.idle": "2024-06-26T03:18:20.704762Z",
     "shell.execute_reply": "2024-06-26T03:18:20.703918Z"
    },
    "papermill": {
     "duration": 0.016881,
     "end_time": "2024-06-26T03:18:20.706590",
     "exception": false,
     "start_time": "2024-06-26T03:18:20.689709",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "not_feature_list = ['essay_id', 'full_text', 'score', 'pos_counts', 'pos_tags']\n",
    "\n",
    "\n",
    "def flatten_features(train_data):\n",
    "\n",
    "    # Extract the feature columns\n",
    "    feature_columns = [\n",
    "        col for col in train_data.columns if col not in not_feature_list]\n",
    "\n",
    "    print(feature_columns)\n",
    "    # Flatten the features for each row\n",
    "    flattened_features = train_data[feature_columns].values.tolist()\n",
    "\n",
    "    # Convert each list of features to a PyTorch tensor\n",
    "    flattened_features_tensors = [torch.tensor(\n",
    "        features, dtype=torch.float32) for features in flattened_features]\n",
    "\n",
    "    # Add the new column to the DataFrame\n",
    "    train_data['flattened_features'] = flattened_features_tensors\n",
    "\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc06c5de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T03:18:20.725607Z",
     "iopub.status.busy": "2024-06-26T03:18:20.725319Z",
     "iopub.status.idle": "2024-06-26T03:18:20.733858Z",
     "shell.execute_reply": "2024-06-26T03:18:20.733039Z"
    },
    "papermill": {
     "duration": 0.020315,
     "end_time": "2024-06-26T03:18:20.735816",
     "exception": false,
     "start_time": "2024-06-26T03:18:20.715501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_all_feature(train_data):\n",
    "\n",
    "    print('extracting all features')\n",
    "    train_data['textstat_features'] = train_data['full_text'].apply(\n",
    "        textstat_features)\n",
    "    train_textstat = pd.DataFrame(train_data['textstat_features'].tolist())\n",
    "    train_data = train_data.drop(columns=['textstat_features'])\n",
    "    train_data = pd.concat([train_data, train_textstat], axis=1)\n",
    "\n",
    "    train_data['spelling_features'] = train_data['full_text'].apply(\n",
    "        spelling_features)\n",
    "    spell_check_df = pd.DataFrame(train_data['spelling_features'].tolist(), columns=[\n",
    "                                  'misspelled_count', 'misspelled_ratio'])\n",
    "    train_data = train_data.drop(columns=['spelling_features'])\n",
    "    train_data = pd.concat([train_data, spell_check_df], axis=1)\n",
    "\n",
    "    train_data['linguistic_features'] = train_data['full_text'].apply(\n",
    "        extract_linguistic_features)\n",
    "    train_linguistic = pd.json_normalize(train_data['linguistic_features'])\n",
    "    train_data = train_data.drop(columns=['linguistic_features'])\n",
    "    train_data = pd.concat([train_data, train_linguistic], axis=1)\n",
    "\n",
    "    train_data['core_features'] = train_data['full_text'].apply(\n",
    "        extract_features)\n",
    "    core_df = pd.DataFrame(train_data['core_features'].tolist())\n",
    "    train_data = train_data.drop(columns=['core_features'])\n",
    "    train_data = pd.concat([train_data, core_df], axis=1)\n",
    "\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96cd03b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T03:18:20.754309Z",
     "iopub.status.busy": "2024-06-26T03:18:20.754053Z",
     "iopub.status.idle": "2024-06-26T03:18:20.759020Z",
     "shell.execute_reply": "2024-06-26T03:18:20.758191Z"
    },
    "papermill": {
     "duration": 0.016351,
     "end_time": "2024-06-26T03:18:20.760858",
     "exception": false,
     "start_time": "2024-06-26T03:18:20.744507",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def select_features(train_data, features=[]):\n",
    "    train_data = train_data.drop(columns=list(\n",
    "        set(train_data.columns)-set(features)))\n",
    "    return train_data\n",
    "\n",
    "\n",
    "def normalize_features(train_data, features_to_normalize=[]):\n",
    "    train_data[features_to_normalize] = minmax_scale(\n",
    "        train_data[features_to_normalize])\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a847fe84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T03:18:20.780066Z",
     "iopub.status.busy": "2024-06-26T03:18:20.779426Z",
     "iopub.status.idle": "2024-06-26T03:18:20.785359Z",
     "shell.execute_reply": "2024-06-26T03:18:20.784477Z"
    },
    "papermill": {
     "duration": 0.017507,
     "end_time": "2024-06-26T03:18:20.787209",
     "exception": false,
     "start_time": "2024-06-26T03:18:20.769702",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_to_use = [\n",
    "    'flesch_reading_ease',\n",
    "    'flesch_kincaid_grade',\n",
    "    'smog_index',\n",
    "    'coleman_liau_index',\n",
    "    'automated_readability_index',\n",
    "    'dale_chall_readability_score',\n",
    "    'difficult_words',\n",
    "    'linsear_write_formula',\n",
    "    'gunning_fog',\n",
    "    'text_standard',\n",
    "    'spache_readability',\n",
    "    'mcalpine_eflaw',\n",
    "    'reading_time',\n",
    "    'syllable_count',\n",
    "    'lexicon_count',\n",
    "    'monosyllabcount',\n",
    "    'misspelled_count',\n",
    "    'misspelled_count',\n",
    "    'past_tense_ratio',\n",
    "    'present_tense_ratio',\n",
    "    'word_count',\n",
    "    'sentence_count',\n",
    "    'words_per_sentence',\n",
    "    'std_words_per_sentence',\n",
    "    'unique_words',\n",
    "    'lexical_diversity',\n",
    "    'paragraph_count',\n",
    "    'avg_chars_by_paragraph',\n",
    "    'avg_words_by_paragraph',\n",
    "    'avg_sentences_by_paragraph',\n",
    "    'num_words',\n",
    "    'num_sentences',\n",
    "    'avg_sentence_length',\n",
    "    'readability',\n",
    "    'num_clauses',\n",
    "    'avg_clause_length',\n",
    "    'unique_words',\n",
    "    \"mean_compound\",\n",
    "    \"mean_negative\",\n",
    "    \"mean_positive\",\n",
    "    \"mean_neutral\",\n",
    "    \"std_compound\",\n",
    "    \"std_negative\",\n",
    "    \"std_positive\",\n",
    "    \"std_neutral\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1c881ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T03:18:20.805717Z",
     "iopub.status.busy": "2024-06-26T03:18:20.805464Z",
     "iopub.status.idle": "2024-06-26T03:18:21.274650Z",
     "shell.execute_reply": "2024-06-26T03:18:21.273855Z"
    },
    "papermill": {
     "duration": 0.480889,
     "end_time": "2024-06-26T03:18:21.276888",
     "exception": false,
     "start_time": "2024-06-26T03:18:20.795999",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['flesch_reading_ease', 'flesch_kincaid_grade', 'smog_index', 'coleman_liau_index', 'automated_readability_index', 'dale_chall_readability_score', 'difficult_words', 'linsear_write_formula', 'gunning_fog', 'text_standard', 'spache_readability', 'mcalpine_eflaw', 'reading_time', 'syllable_count', 'lexicon_count', 'monosyllabcount', 'misspelled_count', 'past_tense_ratio', 'present_tense_ratio', 'word_count', 'sentence_count', 'words_per_sentence', 'std_words_per_sentence', 'unique_words', 'lexical_diversity', 'paragraph_count', 'avg_chars_by_paragraph', 'avg_words_by_paragraph', 'avg_sentences_by_paragraph', 'mean_compound', 'mean_negative', 'mean_positive', 'mean_neutral', 'std_compound', 'std_negative', 'std_positive', 'std_neutral', 'num_words', 'num_sentences', 'avg_sentence_length', 'readability', 'num_clauses', 'avg_clause_length', 'unique_words']\n"
     ]
    }
   ],
   "source": [
    "if not (os.path.exists(processed_data_dir)):\n",
    "    train_data = extract_all_feature(train_data)\n",
    "    train_data.to_csv('aes_train_data_processed.csv')\n",
    "    pickle.dump(train_data, open(\"aes_train_data_processed\", \"wb\"))\n",
    "\n",
    "\n",
    "train_data = select_features(train_data, features_to_use+not_feature_list)\n",
    "train_data = normalize_features(train_data, features_to_use)\n",
    "train_data = flatten_features(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b87ac720",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T03:18:21.296586Z",
     "iopub.status.busy": "2024-06-26T03:18:21.296260Z",
     "iopub.status.idle": "2024-06-26T03:18:21.324879Z",
     "shell.execute_reply": "2024-06-26T03:18:21.324024Z"
    },
    "papermill": {
     "duration": 0.040294,
     "end_time": "2024-06-26T03:18:21.326688",
     "exception": false,
     "start_time": "2024-06-26T03:18:21.286394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a dictionary for POS tags\n",
    "all_pos_tags = set()\n",
    "for i in train_data['pos_counts']:\n",
    "    all_pos_tags.update(i.keys())\n",
    "\n",
    "\n",
    "pos2idx = {tag: idx for idx, tag in enumerate(all_pos_tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94f890db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T03:18:21.345669Z",
     "iopub.status.busy": "2024-06-26T03:18:21.345374Z",
     "iopub.status.idle": "2024-06-26T03:18:21.353685Z",
     "shell.execute_reply": "2024-06-26T03:18:21.352879Z"
    },
    "papermill": {
     "duration": 0.019798,
     "end_time": "2024-06-26T03:18:21.355561",
     "exception": false,
     "start_time": "2024-06-26T03:18:21.335763",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EssayDataset(Dataset):\n",
    "    def __init__(self, data, pos2idx):\n",
    "        self.data = data\n",
    "        self.pos2idx = pos2idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        essay = self.data.iloc[idx]\n",
    "        l = []\n",
    "        for token, pos_tag in essay['pos_tags']:\n",
    "            l.append(self.pos2idx.get(pos_tag, 0))\n",
    "        pos_tags = torch.tensor(l, dtype=torch.long)\n",
    "        flattened_features = essay['flattened_features']\n",
    "        score = torch.tensor(essay['score'], dtype=torch.float32)\n",
    "        return pos_tags, flattened_features, score\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    pos_tags, features, scores = zip(*batch)\n",
    "\n",
    "    pos_tags_padded = nn.utils.rnn.pad_sequence(\n",
    "        pos_tags, batch_first=True, padding_value=0).to(device)\n",
    "\n",
    "    # Stack feature vectors directly (assuming fixed size)\n",
    "    features_padded = torch.stack(features).to(device)\n",
    "\n",
    "    scores = torch.stack(scores).to(device)\n",
    "    return pos_tags_padded, features_padded, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9fa91c79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T03:18:21.374576Z",
     "iopub.status.busy": "2024-06-26T03:18:21.374323Z",
     "iopub.status.idle": "2024-06-26T03:18:21.387909Z",
     "shell.execute_reply": "2024-06-26T03:18:21.387067Z"
    },
    "papermill": {
     "duration": 0.025387,
     "end_time": "2024-06-26T03:18:21.389911",
     "exception": false,
     "start_time": "2024-06-26T03:18:21.364524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attention = nn.Linear(hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, lstm_output):\n",
    "        attn_weights = F.softmax(self.attention(lstm_output), dim=1)\n",
    "        context = torch.bmm(attn_weights.transpose(1, 2), lstm_output)\n",
    "        return context.squeeze(1), attn_weights\n",
    "\n",
    "\n",
    "class EssayScoringModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, feature_dim, dropout_prob):\n",
    "        super(EssayScoringModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(embedding_dim, hidden_dim,\n",
    "                               kernel_size=5, padding='same')\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim,\n",
    "                            batch_first=True, bidirectional=True)\n",
    "        self.lstm_attention = Attention(hidden_dim*2)\n",
    "        self.conv1_attention = Attention(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2 + feature_dim,\n",
    "                             256)  # Increased capacity\n",
    "        self.fc2 = nn.Linear(256, 128)  # Added another fully connected layer\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, pos_tags, features):\n",
    "\n",
    "        # Embedding layer\n",
    "        # Shape: (batch_size, seq_length, num_classes)\n",
    "        embeds = F.one_hot(pos_tags, num_classes=50).float()\n",
    "\n",
    "        # Convolutional layer\n",
    "        # Shape: (batch_size, hidden_dim, seq_length)\n",
    "        conv_out = F.relu(self.conv1(embeds.transpose(1, 2)))\n",
    "        conv_out = self.dropout(conv_out)\n",
    "\n",
    "        # Attention pooling after convolution\n",
    "        attn_conv_out = self.conv1_attention(conv_out.transpose(1, 2))[0]  # Shape: (batch_size, hidden_dim)\n",
    "\n",
    "        # LSTM layer\n",
    "        # Shape: (batch_size, 1, hidden_dim)\n",
    "        lstm_out, _ = self.lstm(attn_conv_out.unsqueeze(1))\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "\n",
    "        # Attention pooling after LSTM\n",
    "        # Shape: (batch_size, hidden_dim * 2)\n",
    "        attn_lstm_out = self.lstm_attention(lstm_out)[0]\n",
    "\n",
    "        # Concatenate LSTM output with additional features\n",
    "        # Shape: (batch_size, hidden_dim * 2 + feature_dim)\n",
    "        combined = torch.cat((attn_lstm_out, features), dim=1)\n",
    "\n",
    "        x = F.relu(self.fc1(combined))  # Shape: (batch_size, 256)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))  # Shape: (batch_size, 128)\n",
    "        x = self.fc3(x)  # Shape: (batch_size, 1)\n",
    "        x = torch.sigmoid(x) * 5 + 1  # scores are between 1 and 6\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd6e2581",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T03:18:21.408669Z",
     "iopub.status.busy": "2024-06-26T03:18:21.408399Z",
     "iopub.status.idle": "2024-06-26T03:18:21.428915Z",
     "shell.execute_reply": "2024-06-26T03:18:21.428085Z"
    },
    "papermill": {
     "duration": 0.031878,
     "end_time": "2024-06-26T03:18:21.430801",
     "exception": false,
     "start_time": "2024-06-26T03:18:21.398923",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training and validation split\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2)\n",
    "train_dataset = EssayDataset(train_data, pos2idx)\n",
    "val_dataset = EssayDataset(val_data, pos2idx)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64,\n",
    "                          shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64,\n",
    "                        shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec0f19d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T03:18:21.450100Z",
     "iopub.status.busy": "2024-06-26T03:18:21.449435Z",
     "iopub.status.idle": "2024-06-26T03:18:23.941502Z",
     "shell.execute_reply": "2024-06-26T03:18:23.940737Z"
    },
    "papermill": {
     "duration": 2.503835,
     "end_time": "2024-06-26T03:18:23.943827",
     "exception": false,
     "start_time": "2024-06-26T03:18:21.439992",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ModelParameters:\n",
    "    def __init__(self, train_data):\n",
    "        self.embedding_dim = 50  # Embedding output dimension\n",
    "        self.hidden_dim = 100  # number of filters (conv)\n",
    "        self.feature_dim = len(train_data['flattened_features'].iloc[0])\n",
    "        self.dropout_prob = 0.5\n",
    "\n",
    "\n",
    "model_parameters = ModelParameters(train_data)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "embedding_dim = model_parameters.embedding_dim\n",
    "hidden_dim = model_parameters.hidden_dim  \n",
    "feature_dim = model_parameters.feature_dim\n",
    "dropout_prob = model_parameters.dropout_prob\n",
    "\n",
    "model = EssayScoringModel(embedding_dim,\n",
    "                          hidden_dim, feature_dim, dropout_prob).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e07be6ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T03:18:23.964185Z",
     "iopub.status.busy": "2024-06-26T03:18:23.963735Z",
     "iopub.status.idle": "2024-06-26T03:32:22.250610Z",
     "shell.execute_reply": "2024-06-26T03:32:22.249391Z"
    },
    "papermill": {
     "duration": 838.299458,
     "end_time": "2024-06-26T03:32:22.252750",
     "exception": false,
     "start_time": "2024-06-26T03:18:23.953292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Train Loss: 0.5914397694273479\n",
      "Valid Loss: 0.5073619977994399\n",
      "saved best model\n",
      "\n",
      "\n",
      "Epoch 2\n",
      "Train Loss: 0.49207359222772484\n",
      "Valid Loss: 0.4811168746514754\n",
      "saved best model\n",
      "\n",
      "\n",
      "Epoch 3\n",
      "Train Loss: 0.4676665589556716\n",
      "Valid Loss: 0.4605275159532374\n",
      "saved best model\n",
      "\n",
      "\n",
      "Epoch 4\n",
      "Train Loss: 0.4478749432322067\n",
      "Valid Loss: 0.4492049986665899\n",
      "saved best model\n",
      "\n",
      "\n",
      "Epoch 5\n",
      "Train Loss: 0.4362673667444062\n",
      "Valid Loss: 0.4293244031342593\n",
      "saved best model\n",
      "\n",
      "\n",
      "Epoch 6\n",
      "Train Loss: 0.4291763000087255\n",
      "Valid Loss: 0.45962025929581035\n",
      "\n",
      "\n",
      "Epoch 7\n",
      "Train Loss: 0.42292727179790973\n",
      "Valid Loss: 0.5373822629451752\n",
      "\n",
      "\n",
      "Epoch 8\n",
      "Train Loss: 0.4126416436538169\n",
      "Valid Loss: 0.42298842235045\n",
      "saved best model\n",
      "\n",
      "\n",
      "Epoch 9\n",
      "Train Loss: 0.4086810921743718\n",
      "Valid Loss: 0.4181366416541013\n",
      "saved best model\n",
      "\n",
      "\n",
      "Epoch 10\n",
      "Train Loss: 0.40302330186839475\n",
      "Valid Loss: 0.4728509073907679\n",
      "\n",
      "\n",
      "Epoch 11\n",
      "Train Loss: 0.40065912105795426\n",
      "Valid Loss: 0.4134326793930747\n",
      "saved best model\n",
      "\n",
      "\n",
      "Epoch 12\n",
      "Train Loss: 0.39602716088569656\n",
      "Valid Loss: 0.4344948641278527\n",
      "\n",
      "\n",
      "Epoch 13\n",
      "Train Loss: 0.39572655413007957\n",
      "Valid Loss: 0.583588820695877\n",
      "\n",
      "\n",
      "Epoch 14\n",
      "Train Loss: 0.39153762429540606\n",
      "Valid Loss: 0.4622565976598046\n",
      "\n",
      "\n",
      "Epoch 15\n",
      "Train Loss: 0.38654324110202526\n",
      "Valid Loss: 0.42229480309919876\n",
      "\n",
      "\n",
      "Epoch 16\n",
      "Train Loss: 0.38614910566312377\n",
      "Valid Loss: 0.4042589171366258\n",
      "saved best model\n",
      "\n",
      "\n",
      "Epoch 17\n",
      "Train Loss: 0.3817538798267391\n",
      "Valid Loss: 0.4341567256233909\n",
      "\n",
      "\n",
      "Epoch 18\n",
      "Train Loss: 0.3796734838029756\n",
      "Valid Loss: 0.3933662880550731\n",
      "saved best model\n",
      "\n",
      "\n",
      "Epoch 19\n",
      "Train Loss: 0.37418173061263177\n",
      "Valid Loss: 0.4504878905686465\n",
      "\n",
      "\n",
      "Epoch 20\n",
      "Train Loss: 0.37315939504155365\n",
      "Valid Loss: 0.4165688547221097\n",
      "\n",
      "\n",
      "Epoch 21\n",
      "Train Loss: 0.3721452155558195\n",
      "Valid Loss: 0.39866425449197945\n",
      "\n",
      "\n",
      "Epoch 22\n",
      "Train Loss: 0.37293217774085735\n",
      "Valid Loss: 1.1205654740333557\n",
      "\n",
      "\n",
      "Epoch 23\n",
      "Train Loss: 0.3696440977434958\n",
      "Valid Loss: 0.4056601345539093\n",
      "\n",
      "\n",
      "Epoch 24\n",
      "Train Loss: 0.3656250604424059\n",
      "Valid Loss: 0.43616922985423695\n",
      "\n",
      "\n",
      "Epoch 25\n",
      "Train Loss: 0.36501961115593184\n",
      "Valid Loss: 0.3863183790987188\n",
      "saved best model\n",
      "\n",
      "\n",
      "Epoch 26\n",
      "Train Loss: 0.362397439262834\n",
      "Valid Loss: 0.3909438994797793\n",
      "\n",
      "\n",
      "Epoch 27\n",
      "Train Loss: 0.360177496611248\n",
      "Valid Loss: 0.389088006994941\n",
      "\n",
      "\n",
      "Epoch 28\n",
      "Train Loss: 0.3554791698258044\n",
      "Valid Loss: 0.4636253589933569\n",
      "\n",
      "\n",
      "Epoch 29\n",
      "Train Loss: 0.357881081269084\n",
      "Valid Loss: 0.4066979478706013\n",
      "\n",
      "\n",
      "Epoch 30\n",
      "Train Loss: 0.35611385855531913\n",
      "Valid Loss: 0.3879971761595119\n",
      "\n",
      "\n",
      "Epoch 31\n",
      "Train Loss: 0.35344387384878323\n",
      "Valid Loss: 0.40190604058178986\n",
      "\n",
      "\n",
      "Epoch 32\n",
      "Train Loss: 0.34884804500962185\n",
      "Valid Loss: 0.38337483297694813\n",
      "saved best model\n",
      "\n",
      "\n",
      "Epoch 33\n",
      "Train Loss: 0.3474466331680799\n",
      "Valid Loss: 0.4227981784126975\n",
      "\n",
      "\n",
      "Epoch 34\n",
      "Train Loss: 0.3469161705075321\n",
      "Valid Loss: 0.3981956373561512\n",
      "\n",
      "\n",
      "Epoch 35\n",
      "Train Loss: 0.3479116957995199\n",
      "Valid Loss: 0.3811703635887666\n",
      "saved best model\n",
      "\n",
      "\n",
      "Epoch 36\n",
      "Train Loss: 0.34433917007688003\n",
      "Valid Loss: 0.39549714137207376\n",
      "\n",
      "\n",
      "Epoch 37\n",
      "Train Loss: 0.34162692121371696\n",
      "Valid Loss: 0.4290872264992107\n",
      "\n",
      "\n",
      "Epoch 38\n",
      "Train Loss: 0.3415036388943272\n",
      "Valid Loss: 0.3786161059683019\n",
      "saved best model\n",
      "\n",
      "\n",
      "Epoch 39\n",
      "Train Loss: 0.3417844205820066\n",
      "Valid Loss: 0.40012626268646934\n",
      "\n",
      "\n",
      "Epoch 40\n",
      "Train Loss: 0.33714828959533144\n",
      "Valid Loss: 0.38391795944083823\n",
      "\n",
      "\n",
      "Epoch 41\n",
      "Train Loss: 0.3388756295228334\n",
      "Valid Loss: 0.42311276901852\n",
      "\n",
      "\n",
      "Epoch 42\n",
      "Train Loss: 0.3375206450987521\n",
      "Valid Loss: 0.38043832020326096\n",
      "\n",
      "\n",
      "Epoch 43\n",
      "Train Loss: 0.3351905639819835\n",
      "Valid Loss: 0.4162797364321622\n",
      "\n",
      "\n",
      "Epoch 44\n",
      "Train Loss: 0.3297049855718964\n",
      "Valid Loss: 0.3952029287815094\n",
      "\n",
      "\n",
      "Epoch 45\n",
      "Train Loss: 0.33027335383375667\n",
      "Valid Loss: 0.41236593560739\n",
      "\n",
      "\n",
      "Epoch 46\n",
      "Train Loss: 0.3302664947674571\n",
      "Valid Loss: 0.43779620257290924\n",
      "\n",
      "\n",
      "Epoch 47\n",
      "Train Loss: 0.3305197139077472\n",
      "Valid Loss: 0.49557205438613894\n",
      "\n",
      "\n",
      "Epoch 48\n",
      "Train Loss: 0.32829410469477077\n",
      "Valid Loss: 0.3893225480209697\n",
      "\n",
      "\n",
      "Epoch 49\n",
      "Train Loss: 0.32398611636755104\n",
      "Valid Loss: 0.6174795372919603\n",
      "\n",
      "\n",
      "Epoch 50\n",
      "Train Loss: 0.3240246290023426\n",
      "Valid Loss: 0.38646175563335416\n",
      "\n",
      "\n",
      "Epoch 51\n",
      "Train Loss: 0.3242766346936951\n",
      "Valid Loss: 0.40521793528036637\n",
      "\n",
      "\n",
      "Epoch 52\n",
      "Train Loss: 0.3234565282060254\n",
      "Valid Loss: 0.37755104194987904\n",
      "saved best model\n",
      "\n",
      "\n",
      "Epoch 53\n",
      "Train Loss: 0.3211186318078898\n",
      "Valid Loss: 0.39375304931944066\n",
      "\n",
      "\n",
      "Epoch 54\n",
      "Train Loss: 0.32063514983049735\n",
      "Valid Loss: 0.44438320886005056\n",
      "\n",
      "\n",
      "Epoch 55\n",
      "Train Loss: 0.319448529094595\n",
      "Valid Loss: 0.39436313780871307\n",
      "\n",
      "\n",
      "Epoch 56\n",
      "Train Loss: 0.3132086291840549\n",
      "Valid Loss: 0.46921471574089746\n",
      "\n",
      "\n",
      "Epoch 57\n",
      "Train Loss: 0.3143758444192772\n",
      "Valid Loss: 0.4114577691663395\n",
      "\n",
      "\n",
      "Epoch 58\n",
      "Train Loss: 0.31546850619228206\n",
      "Valid Loss: 0.3866954871199348\n",
      "\n",
      "\n",
      "Epoch 59\n",
      "Train Loss: 0.3154833727443273\n",
      "Valid Loss: 0.5937307520346208\n",
      "\n",
      "\n",
      "Epoch 60\n",
      "Train Loss: 0.31359547747445\n",
      "Valid Loss: 0.389981239492243\n",
      "\n",
      "\n",
      "Epoch 61\n",
      "Train Loss: 0.31198145160751956\n",
      "Valid Loss: 0.40924109437248923\n",
      "\n",
      "\n",
      "Epoch 62\n",
      "Train Loss: 0.31198894057405707\n",
      "Valid Loss: 0.3982916203412143\n",
      "\n",
      "\n",
      "Epoch 63\n",
      "Train Loss: 0.30796692908359563\n",
      "Valid Loss: 0.40638752525502986\n",
      "\n",
      "\n",
      "Epoch 64\n",
      "Train Loss: 0.30799269119990036\n",
      "Valid Loss: 0.47289170080965215\n",
      "\n",
      "\n",
      "Epoch 65\n",
      "Train Loss: 0.30681901538426976\n",
      "Valid Loss: 0.394097684730183\n",
      "\n",
      "\n",
      "Epoch 66\n",
      "Train Loss: 0.3058006372731952\n",
      "Valid Loss: 0.4981362499973991\n",
      "\n",
      "\n",
      "Epoch 67\n",
      "Train Loss: 0.30440554318065466\n",
      "Valid Loss: 0.40856844240968876\n",
      "\n",
      "\n",
      "Epoch 68\n",
      "Train Loss: 0.3052350593052702\n",
      "Valid Loss: 0.9526171456683766\n",
      "\n",
      "\n",
      "Epoch 69\n",
      "Train Loss: 0.30582520770861804\n",
      "Valid Loss: 0.38200522607023063\n",
      "\n",
      "\n",
      "Epoch 70\n",
      "Train Loss: 0.30280599485619275\n",
      "Valid Loss: 0.4253637416796251\n",
      "\n",
      "\n",
      "Epoch 71\n",
      "Train Loss: 0.29924296659808003\n",
      "Valid Loss: 0.39170155389742417\n",
      "\n",
      "\n",
      "Epoch 72\n",
      "Train Loss: 0.29780261740431807\n",
      "Valid Loss: 0.5744940811937506\n",
      "\n",
      "\n",
      "Epoch 73\n",
      "Train Loss: 0.30183804138190184\n",
      "Valid Loss: 0.45385740236802535\n",
      "\n",
      "\n",
      "Epoch 74\n",
      "Train Loss: 0.2974009812564894\n",
      "Valid Loss: 0.4137596000324596\n",
      "\n",
      "\n",
      "Epoch 75\n",
      "Train Loss: 0.2975423277660449\n",
      "Valid Loss: 0.4061468384482644\n",
      "\n",
      "\n",
      "Epoch 76\n",
      "Train Loss: 0.2915481287762866\n",
      "Valid Loss: 0.41255410476164384\n",
      "\n",
      "\n",
      "Epoch 77\n",
      "Train Loss: 0.2985039516802757\n",
      "Valid Loss: 0.4611034301194278\n",
      "\n",
      "\n",
      "Epoch 78\n",
      "Train Loss: 0.29296811467491535\n",
      "Valid Loss: 0.39884877584197304\n",
      "\n",
      "\n",
      "Epoch 79\n",
      "Train Loss: 0.29195271768877584\n",
      "Valid Loss: 0.4283672565763647\n",
      "\n",
      "\n",
      "Epoch 80\n",
      "Train Loss: 0.28938297893045134\n",
      "Valid Loss: 0.4344280091199008\n",
      "\n",
      "\n",
      "Epoch 81\n",
      "Train Loss: 0.2934586159919264\n",
      "Valid Loss: 0.48048222715204414\n",
      "\n",
      "\n",
      "Epoch 82\n",
      "Train Loss: 0.29239920413439174\n",
      "Valid Loss: 0.41003744873133574\n",
      "\n",
      "\n",
      "Epoch 83\n",
      "Train Loss: 0.2891795287758524\n",
      "Valid Loss: 0.42205930027094757\n",
      "\n",
      "\n",
      "Epoch 84\n",
      "Train Loss: 0.29210739304667793\n",
      "Valid Loss: 0.920398724079132\n",
      "\n",
      "\n",
      "Epoch 85\n",
      "Train Loss: 0.2917863916828885\n",
      "Valid Loss: 0.4416756277734583\n",
      "\n",
      "\n",
      "Epoch 86\n",
      "Train Loss: 0.288129023555237\n",
      "Valid Loss: 0.5595625904473391\n",
      "\n",
      "\n",
      "Epoch 87\n",
      "Train Loss: 0.29109776796009135\n",
      "Valid Loss: 0.4167507464235479\n",
      "\n",
      "\n",
      "Epoch 88\n",
      "Train Loss: 0.2881885109134533\n",
      "Valid Loss: 0.40135857517069035\n",
      "\n",
      "\n",
      "Epoch 89\n",
      "Train Loss: 0.2858003545466656\n",
      "Valid Loss: 0.4046635470607064\n",
      "\n",
      "\n",
      "Epoch 90\n",
      "Train Loss: 0.28694290238591386\n",
      "Valid Loss: 0.5097774511033838\n",
      "\n",
      "\n",
      "Epoch 91\n",
      "Train Loss: 0.2827670745173907\n",
      "Valid Loss: 0.488098928603259\n",
      "\n",
      "\n",
      "Epoch 92\n",
      "Train Loss: 0.28163053363149615\n",
      "Valid Loss: 0.3860371053218842\n",
      "\n",
      "\n",
      "Epoch 93\n",
      "Train Loss: 0.2887366026777276\n",
      "Valid Loss: 0.4041400654749437\n",
      "\n",
      "\n",
      "Epoch 94\n",
      "Train Loss: 0.28272266782099204\n",
      "Valid Loss: 0.3911025881767273\n",
      "\n",
      "\n",
      "Epoch 95\n",
      "Train Loss: 0.284453060838484\n",
      "Valid Loss: 0.3949183198538693\n",
      "\n",
      "\n",
      "Epoch 96\n",
      "Train Loss: 0.2772399189148081\n",
      "Valid Loss: 0.395052587444132\n",
      "\n",
      "\n",
      "Epoch 97\n",
      "Train Loss: 0.2803966343952214\n",
      "Valid Loss: 0.4167792461135171\n",
      "\n",
      "\n",
      "Epoch 98\n",
      "Train Loss: 0.2822556760728634\n",
      "Valid Loss: 0.43516439687122\n",
      "\n",
      "\n",
      "Epoch 99\n",
      "Train Loss: 0.2753235936027518\n",
      "Valid Loss: 0.3906023155559193\n",
      "\n",
      "\n",
      "Epoch 100\n",
      "Train Loss: 0.27685916087319773\n",
      "Valid Loss: 0.38885924436829306\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lowest_val_loss = 10**5\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for pos_tags, features, scores in train_loader:\n",
    "        pos_tags, featuers, scores = pos_tags.to(\n",
    "            device), features.to(device), scores.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(pos_tags, features).squeeze()\n",
    "        loss = criterion(outputs, scores)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}')\n",
    "    print(f'Train Loss: {running_loss / len(train_loader)}')\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for pos_tags, features, scores in val_loader:\n",
    "            pos_tags, featuers, scores = pos_tags.to(\n",
    "                device), features.to(device), scores.to(device)\n",
    "            outputs = model(pos_tags, features).squeeze()\n",
    "            loss = criterion(outputs, scores)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print(f'Valid Loss: {val_loss / len(val_loader)}')\n",
    "\n",
    "    # save best model weights\n",
    "    if val_loss / len(val_loader) <= lowest_val_loss:\n",
    "        lowest_val_loss = val_loss / len(val_loader)\n",
    "        torch.save(model.state_dict(), 'essay_scoring_best_model.pth')\n",
    "        print('saved best model')\n",
    "\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1114c02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T03:32:22.302771Z",
     "iopub.status.busy": "2024-06-26T03:32:22.302449Z",
     "iopub.status.idle": "2024-06-26T03:32:23.938760Z",
     "shell.execute_reply": "2024-06-26T03:32:23.937610Z"
    },
    "papermill": {
     "duration": 1.663791,
     "end_time": "2024-06-26T03:32:23.941075",
     "exception": false,
     "start_time": "2024-06-26T03:32:22.277284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quadratic Weighted Kappa: 0.7659923925088844\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "    all_scores = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for pos_tags, features, scores in loader:\n",
    "            outputs = model(pos_tags, features).squeeze()\n",
    "            all_scores.extend(scores.tolist())\n",
    "            all_preds.extend(outputs.tolist())\n",
    "\n",
    "    all_scores = [round(score) for score in all_scores]\n",
    "    all_preds = [round(pred) for pred in all_preds]\n",
    "\n",
    "    qwk_score = cohen_kappa_score(all_scores, all_preds, weights='quadratic')\n",
    "    return qwk_score\n",
    "\n",
    "\n",
    "# Load and Evaluate best model\n",
    "model.load_state_dict(torch.load(\n",
    "    '/kaggle/working/essay_scoring_best_model.pth'))\n",
    "\n",
    "qwk_score = evaluate_model(model, val_loader)\n",
    "print(f'Quadratic Weighted Kappa: {qwk_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0993886e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T03:32:23.994040Z",
     "iopub.status.busy": "2024-06-26T03:32:23.993423Z",
     "iopub.status.idle": "2024-06-26T03:32:23.998638Z",
     "shell.execute_reply": "2024-06-26T03:32:23.997783Z"
    },
    "papermill": {
     "duration": 0.032942,
     "end_time": "2024-06-26T03:32:24.000766",
     "exception": false,
     "start_time": "2024-06-26T03:32:23.967824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pickle.dump(pos2idx, open(\"pos2idx\", \"wb\"))\n",
    "pickle.dump(model_parameters, open(\"model_parameters\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb07abf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T03:32:24.118936Z",
     "iopub.status.busy": "2024-06-26T03:32:24.118500Z",
     "iopub.status.idle": "2024-06-26T03:32:24.124172Z",
     "shell.execute_reply": "2024-06-26T03:32:24.123136Z"
    },
    "papermill": {
     "duration": 0.086766,
     "end_time": "2024-06-26T03:32:24.126440",
     "exception": false,
     "start_time": "2024-06-26T03:32:24.039674",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip download textstat -d ./textstat/\n",
    "# !pip download vaderSentiment -d ./vaderSentiment/\n",
    "# !pip download pyspellchecker -d ./pyspellchecker/\n",
    "\n",
    "# import os\n",
    "# from zipfile import ZipFile\n",
    "\n",
    "# dirName = \"./\"\n",
    "# zipName = \"packages.zip\"\n",
    "\n",
    "# # Create a ZipFile Object\n",
    "# with ZipFile(zipName, 'w') as zipObj:\n",
    "#     # Iterate over all the files in directory\n",
    "#     for folderName, subfolders, filenames in os.walk(dirName):\n",
    "#         for filename in filenames:\n",
    "#             if (filename != zipName):\n",
    "#                 # create complete filepath of file in directory\n",
    "#                 filePath = os.path.join(folderName, filename)\n",
    "#                 # Add file to zip\n",
    "#                 zipObj.write(filePath)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8059942,
     "sourceId": 71485,
     "sourceType": "competition"
    },
    {
     "datasetId": 5074452,
     "sourceId": 8573639,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 901.505301,
   "end_time": "2024-06-26T03:32:27.167352",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-26T03:17:25.662051",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
